{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text\n",
        "!pip install sentence_transformers\n",
        "!pip install profanity-check\n",
        "!pip install transformers\n",
        "!pip install better_profanity"
      ],
      "metadata": {
        "id": "or6zSEpSYAqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from better_profanity import profanity\n",
        "profanity.censor(\"fuck off\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2xYNwRaXiGRB",
        "outputId": "828e5c6e-ca65-4bc0-b900-c97f723e956e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'****'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask_cors"
      ],
      "metadata": {
        "id": "YevyQx1NYPDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask_ngrok\n",
        "!pip install pyngrok==4.1.1\n",
        "\n",
        "!ngrok authtoken 2PUA67uueLD6KUjmBkEW2XjZZSe_2rt4FLAvfahfaZrJH8iCc"
      ],
      "metadata": {
        "id": "Jm-nNJK8bkRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "import pandas as pd\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "from torch.optim import Adam\n",
        "from tensorflow import keras\n",
        "import tensorflow_text as text\n",
        "from flask import Flask, render_template, request, jsonify\n",
        "from flask_cors import CORS, cross_origin\n",
        "import urllib\n",
        "import json\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import csv\n",
        "import pickle\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os"
      ],
      "metadata": {
        "id": "fCyvzCfWbvQT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_tokens([\"<reply>:\"])\n",
        "tokenizer.add_tokens([\"<sentiment>:\"])\n",
        "tokenizer.add_tokens([\"<ner>:\"])\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \n",
        "                                \"bos_token\": \"<startofstring>\",\n",
        "                                \"eos_token\": \"<endofstring>\"})\n",
        "model_gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model_gpt2.resize_token_embeddings(len(tokenizer))\n",
        "model_gpt2 = model_gpt2.to(device)"
      ],
      "metadata": {
        "id": "qC_1i1n5ljm_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/NLP-Project/sentence_embeddings.pkl', 'rb') as f:\n",
        "    sentence_embeddings = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP-Project/df_all.csv', newline='', encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    next(reader)  # skip the header row\n",
        "    qa_data = [{\"question\": row[1], \"answer\": row[2]} for row in reader]"
      ],
      "metadata": {
        "id": "CLThzMzj1EOw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_sim = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "metadata": {
        "id": "jumd59wH4S-A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "\n",
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)"
      ],
      "metadata": {
        "id": "cBM2UNtD5Dic"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ret_roberta(query, res_gpt, last_line):\n",
        "  q_embed = model_sim.encode(query)\n",
        "  results = cosine_similarity(\n",
        "      [q_embed],\n",
        "      sentence_embeddings\n",
        "  )\n",
        "  res_gpt = res_gpt.split(\"<reply>:\")\n",
        "  res_gpt = res_gpt[1]\n",
        "  print(\"GPT reply:\", res_gpt)\n",
        "  \n",
        "  \n",
        "  if \"Reply: \" in last_line:\n",
        "    last_line = last_line.split(\"Reply: \")\n",
        "    last_line = last_line[1]\n",
        "  \n",
        "  # largest\n",
        "  larg = np.argpartition(results[0], -1)[-1]\n",
        "\n",
        "  #second largest\n",
        "  sec_lar = np.argpartition(results[0], -2)[-2]\n",
        "  th_lar = np.argpartition(results[0], -2)[-3]\n",
        "\n",
        "  combined_context = qa_data[larg]['question'] + '. ' + qa_data[sec_lar]['question']+'. '+qa_data[larg]['answer'] + '. ' + qa_data[sec_lar]['answer'] +'. ' + qa_data[th_lar]['question']+'. ' + qa_data[th_lar]['answer']\n",
        "  print(\"context: \", query, last_line, res_gpt, combined_context)\n",
        "  QA_input = {\n",
        "      'question': query,\n",
        "      'context': query + res_gpt + combined_context\n",
        "  }\n",
        "  link = \"\"\n",
        "  res = nlp(QA_input)\n",
        "  for word in combined_context.split(\" \"):\n",
        "    if \"https\" in word:\n",
        "        link = \" link: \" + word\n",
        "  return res[\"answer\"] + link"
      ],
      "metadata": {
        "id": "tXUrZuuf1m7H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ret_roberta(\"why do you think apple is cool\",\" I'll be quick to anger if I don't know her names.\")"
      ],
      "metadata": {
        "id": "8Pkq-AIU1m4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "cors = CORS(app)\n",
        "app.config['CORS_HEADERS'] = 'Content-Type'\n",
        "\n",
        "model_class = keras.models.load_model(\"/content/drive/MyDrive/NLP-Project/classmodel_newfull2\")\n",
        "model_gpt2.load_state_dict(torch.load('/content/drive/MyDrive/NLP-Project/model_state_ner_sent2.pt',map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "w_-jK72vsqVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3027b39e-b477-4c67-ca2b-b5b205ac73c5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TzeDbLx5X9zj"
      },
      "outputs": [],
      "source": [
        "def infer(inp):\n",
        "    inp = \"<startofstring> \"+inp+\" <reply>: \"\n",
        "    inp = tokenizer(inp, return_tensors=\"pt\")\n",
        "    X = inp[\"input_ids\"].to(device)\n",
        "    a = inp[\"attention_mask\"].to(device)\n",
        "    output = model_gpt2.generate(X, attention_mask=a )\n",
        "    output = tokenizer.decode(output[0])\n",
        "    return output\n",
        "\n",
        "def getModel(query):\n",
        "    coreidnp = model_class.predict([query])\n",
        "    # print(coreidnp)\n",
        "    coreidnp = np.where(coreidnp > 0.25, 1, 0)\n",
        "    coreid = coreidnp[0][0]\n",
        "    if coreid == 0:\n",
        "        mod = 'gpt2'\n",
        "    else:\n",
        "        mod = 'roberta'\n",
        "    # print(\"model: \", mod)\n",
        "    return mod\n",
        "\n",
        "@app.route(\"/\")\n",
        "# @cross_origin()\n",
        "def home():\n",
        "    return render_template('index.html')\n",
        "\n",
        "def get_reply(query, last_line):\n",
        "  try:\n",
        "    query = query\n",
        "    reply_gpt = infer(query)\n",
        "    reply_roberta = ret_roberta(query, reply_gpt, last_line)\n",
        "\n",
        "    print(\"Reply:\", reply_roberta)\n",
        "    if profanity.contains_profanity(query):\n",
        "        return profanity.censor(reply_roberta)\n",
        "    if profanity.contains_profanity(query):\n",
        "        return \"Please reframe your question with appropriate language!\"\n",
        "    if reply_roberta == \"\":\n",
        "        return \"Sorry I did not understand!\"\n",
        "    \n",
        "    return reply_roberta\n",
        "    \n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return \"Sorry I did not understand!\"\n",
        "\n",
        "@app.route('/inputQuery', methods=['POST'])\n",
        "@cross_origin()\n",
        "def getInput():\n",
        "    try:\n",
        "        last_line = \"\"\n",
        "        data = request.get_json()\n",
        "        filename = \"/content/drive/MyDrive/NLP-Project/Queries.txt\"\n",
        "        if os.path.getsize(filename) > 0:\n",
        "          with open(filename, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            last_line = lines[-1].strip()\n",
        "\n",
        "        reply = get_reply(data[\"query\"],last_line)\n",
        "        topic_list = data[\"list_of_topics\"]\n",
        "        # print(topic_list)\n",
        "        with open(\"/content/drive/MyDrive/NLP-Project/Queries.txt\", \"a\") as file:\n",
        "          file.write(\"Query: \"+ data[\"query\"])\n",
        "          file.write(\"Reply: \"+ reply +\"\\n\")\n",
        "        return reply\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return \"Sorry I did not understand!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getModel(\"covid vaccine\")"
      ],
      "metadata": {
        "id": "JZGECEkEpHVM"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infer(\"who is donal trump ? President Trump\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "GL5dw9VoGX4F",
        "outputId": "f2fc76fe-d192-44e4-d8c6-32591ede1400"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<startofstring> who is donal trump? President Trump <reply>: they have lots of good young adults that are coming'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_reply(\"who is donald trump ?\",\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "iQkIF3UYfu1c",
        "outputId": "97e828bf-e4f2-4abf-ea1f-654e4519e869"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT reply:  they'll all have terrible con stories <endofstring> <ner>: <sentiment>: neutral <pad>\n",
            "context:  who is donald trump ?   they'll all have terrible con stories <endofstring> <ner>: <sentiment>: neutral <pad> Maybe I can ask Donald Trump for permission. Trump: An American Dream. thats in the US ya but hes a really stickler. I'll have to look it up something else I ran into recently was about the hole in the ozone layer. do you remember that thing?. What thats crazy how could he change the constitution But I mean if any president would do that it would be President Donald Trump He is kinda crazy to be honest. Yeah I agree he has done some really crazy things and cna be kinda out of control sometimes\n",
            "Reply: President Donald Trump\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'President Donald Trump'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    app.run()"
      ],
      "metadata": {
        "id": "_wPPVYGcYKhK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a9944c-2154-41ca-fd2e-a330510f1f3b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Running on http://5fa8-34-168-210-222.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:32:42] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:32:42] \"GET /static/styles/style.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:32:42] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  Hello! How are you? <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "context:  hello how's it going?.  Hello! How are you? <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> Hello. Hello. Hey there!. Hey, how's it going?. Hello. Hey there!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:33:11] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: how's it going?.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:55:26] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:55:26] \"\u001b[36mGET /static/styles/style.css HTTP/1.1\u001b[0m\" 304 -\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  Hello! How are you? <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "context:  hello how's it going?.  Hello! How are you? <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> Hello. Hello. Hey there!. Hey, how's it going?. Hello. Hey there!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:55:33] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: how's it going?.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  I'm great! How are you? <pad> <pad> <pad> <pad>\n",
            "context:  good, how are you ? how's it going?.  I'm great! How are you? <pad> <pad> <pad> <pad> Good, how are you?. Good, how are you?. Great thanks. Did you do anything for mother's day yesterday?. Great thanks. Did you do anything for mother's day yesterday?. Good! How are you?. I'm great! Thanks!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:55:45] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: I'm great\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  I love the feeling of the spirit, you know what I mean\n",
            "context:  do you like music ? I'm great  I love the feeling of the spirit, you know what I mean What music do you like?. What kind of music do you like?. My playlists consist of 80's, Disney, Soundtracks, and Christmas music you?. A lot.... haha Pop, country, latin, etc. What kind of music do you like?. I listen to a lot of EDM and chillstep. And alternative\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:56:13] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: I love the feeling of the spirit\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  Haha, I love A Million Dreams because of its effects\n",
            "context:  what is your favourite song ? I love the feeling of the spirit  Haha, I love A Million Dreams because of its effects what is your favorite song. yeah What is your favorite song by them?. Mmm hard to say i like a lot of music. Maybe The Mission/How Great Thou Art?  It isn't their most ambitious song or the coolest technically but I love, love, love The Mission soundtrack and of course the setting is also beautiful. Hey same Do you have a favorite song or artist?. Yes? Right now I really like shape of you by ed sheeran That has been stuck in my head But AJR is awesome, I like their stuff\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:56:32] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: A Million Dreams\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  they have a week left. <endofstring> <ner>: <sentiment>: neutral <pad> <pad>\n",
            "context:  who is donal trump ? A Million Dreams  they have a week left. <endofstring> <ner>: <sentiment>: neutral <pad> <pad> Trump: An American Dream. Maybe I can ask Donald Trump for permission. I'll have to look it up something else I ran into recently was about the hole in the ozone layer. do you remember that thing?. thats in the US ya but hes a really stickler. Quick question before I leave. What do you think of President Trump? Just curious.. You know? I think he's unorthodox as a president and says things that are immature for his age, but he's done a lot for this economy. Not my favorite president, but we could be doing a lot worse.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:57:01] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: President Trump\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  they'll all have terrible con stories <endofstring> <ner>: <sentiment>: neutral <pad>\n",
            "context:  who is donald trump ? President Trump  they'll all have terrible con stories <endofstring> <ner>: <sentiment>: neutral <pad> Maybe I can ask Donald Trump for permission. Trump: An American Dream. thats in the US ya but hes a really stickler. I'll have to look it up something else I ran into recently was about the hole in the ozone layer. do you remember that thing?. What thats crazy how could he change the constitution But I mean if any president would do that it would be President Donald Trump He is kinda crazy to be honest. Yeah I agree he has done some really crazy things and cna be kinda out of control sometimes\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:57:12] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: President Donald Trump\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  Yes! I always thought they were awesome but I didn't\n",
            "context:  are covid vaccines good ? President Donald Trump  Yes! I always thought they were awesome but I didn't So you've provided examples of vaccines that were effective. Remind me again what the masking policy was for those diseases?. So you don't believe the vaccine is effective? Interesting.. &gt; I'm unaware of ANY illness that we've come to expect 0 loss of life from except COVID apparently. We're instituting policies to protect who exactly?\n",
            "\n",
            "So you are admitting this comment has no basis in reality then, and just changing the topic?. So you don't think that masking and social distancing also helps? Interesting.. How does this make any sense? Vaccines don’t stop infection.. Just…sigh. I don’t even see a point anymore.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:57:34] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: Yes!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  yeah they are great they are great they are fun to play\n",
            "context:  are covid vaccines bad ? Yes!  yeah they are great they are great they are fun to play Alright let's say that some prominent skeptics are abnormally biased against vaccines. So what?. So you don't believe the vaccine is effective? Interesting.. Are vaccines equivalent to the credibility of the Nigerian royal house?\n",
            "\n",
            "Or has one of them repeatedly spammed billions of inboxes on the regular?\n",
            "\n",
            "This is the entire problem of anti-vaxxinations.  It is true that scientific truth isn't 100%.  That doesn't make the credibility of random claims the same as scientific consensus.\n",
            "\n",
            "The fact that scientific truth isn't 100%, doesn't make your random facebook uncle of the same dependability of science.. So you don't think that masking and social distancing also helps? Interesting.. &gt; How is it fair, are there any studies suggesting ivermetcin works against covid in humans?\n",
            "\n",
            "yes.  They get criticized either for having too small a sample, or by accusations that the data is fabricated.. There are several that I have seen.  The results are ambiguous.  The one study that showed significant positive results was retracted because it was found to have had fabricated data.  Here is a meta-analysis:  https://www.nature.com/articles/s41591-021-01535-y\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:57:45] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: yeah they are great link: https://www.nature.com/articles/s41591-021-01535-y\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  yeah they are great they are great they are fun to play\n",
            "context:  are covid vaccines effective ? yeah they are great link: https://www.nature.com/articles/s41591-021-01535-y  yeah they are great they are great they are fun to play So you've provided examples of vaccines that were effective. Remind me again what the masking policy was for those diseases?. How does this make any sense? Vaccines don’t stop infection.. &gt; I'm unaware of ANY illness that we've come to expect 0 loss of life from except COVID apparently. We're instituting policies to protect who exactly?\n",
            "\n",
            "So you are admitting this comment has no basis in reality then, and just changing the topic?. No one ever said it would stop infection. It prevents hospitalization, death and slows spread. You’ve been brainwashed. Don’t claim to believe in freedom if you’ve let them take yours away. You’re being lied to by people who don’t care about you.. How does this make any sense? Vaccines don’t stop infection.. They can lessen the disease and in turn the spread due to lower viral loads. If we had reached herd immunity numbers with the first rollout with the vaccine, we would be living in a different world.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:58:10] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: yeah they are great\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  yeah but it's a huge perk! I'll be in\n",
            "context:  are covid vaccines out? yeah they are great  yeah but it's a huge perk! I'll be in OK so you aren't qplus17, OP is. Still, not much sense in the analogy. Are you saying that\n",
            "\n",
            "I'm the vaccine, qplus17 is the placebo?\n",
            "\n",
            "Or\n",
            "\n",
            "Qplus17 is the vaccine, I'm the control group\n",
            "\n",
            "Or\n",
            "\n",
            "Me and qplus17 are both prior studies, you are the actual vaccine?\n",
            "\n",
            "What are you actually saying?. I said \"Look up the \"vaccine placebo pyramid scheme\"\"\n",
            "\n",
            "Did you do that?. NO.\n",
            "\n",
            "I'm not talking about covid vaccines - I said that most vaccines were never tested against placebo. Which they aren't.\n",
            "\n",
            "Covid jabs:\n",
            "\n",
            "Astra Zeneca was tested against a dangerous vaccine, meningitis I think, which itself has never been tested against placebo.\n",
            "\n",
            "Pfizer was tested against a saline placebo, and the overall death rate was higher in the jabbed group. ONE fewer died of covid but more had heart attacks - I wonder why.\n",
            "\n",
            "Then as soon as they could they vaccinated the control group, and more died.\n",
            "\n",
            "Start again.\n",
            "\n",
            "Forget everything you know, or think you know, and go back and look at all of this with a fresh and open mind.. Ok. For some reason I can't find your last comment about the vaccine placebo pyramid scheme video, so I'll just have to respond here. I'm not at work now (I work at a college btw, and studied at university after finishing school, so be assured my level of education is sufficient) so I have a bit more time to go into detail as to why I'm not terrified after watching it. \n",
            "\n",
            "I didn't watch it initially for a few reasons. The main one being, as I said, I prefer to read about things rather than watch YouTube videos about them (that is not usually a sign of poor education). I also didn't initially watch it because you implied in your original comment that it would be hard to find info about the vaccine placebo pyramid scheme simply by googling it, and that video is on the first page of results, eight down from the top. So I looked a little more to find some more credible looking information. I found a lot to read on vaccine trials - I'm not going to link it all for you, because I seriously doubt you are going to change your views on this, so I'm not even going to begin attempting to help you with that. As I correctly stated in a previous comment, the term 'placebo' can be and often is used to refer to what's given in trials regardless of whether that placebo is essentially nothing, like saline, or what they call an 'active placebo', which is often, as you correctly state, a vaccine, or an alternative medication. The video makes a very basic point about this, illustrated in a simple diagram, and is full of reactionary and opinionated language claiming that because of this process, no vaccines are are safe, never have been, and never will be:\n",
            "\n",
            "'this is so dangerous, we have no idea, our entire health department, not just of this nation, but of the entire world, has zero idea, whether these are safe or not'. \n",
            "\n",
            "That is a direct quote from the video. It is wrong, on so many levels. There are many reasons for using an active placebo. Again, I'm not going to go into that in detail, as the saying goes 'do your own research'. But I would strongly encourage you to read some papers, actual studies and work done by scientists, NOT youtube videos with less than a thousand views from a conspiracy website. Because they are full of opinion, and science is based on fact. As soon as you introduce opinion and start trying to make people scared you loose all integrity. I guarantee that the majority of 'academics' you sent that to watched it, cringed, laughed at you, and moved on.\n",
            "\n",
            "I would like to thank you for this though - I am working on a project around disinformation, conspiracy, how it manifests and spreads with a group of my students. This video, and the I can decide.org website, are going to make excellent resources for that project.. I said \"Look up the \"vaccine placebo pyramid scheme\"\"\n",
            "\n",
            "Did you do that?. Yes\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:58:35] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: no vaccines are are safe\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  yeah I did not know that. I thought that they were\n",
            "context:  are covid vaccines safe? no vaccines are are safe  yeah I did not know that. I thought that they were How does this make any sense? Vaccines don’t stop infection.. How does this make any sense? Vaccines don’t stop infection.. They can lessen the disease and in turn the spread due to lower viral loads. If we had reached herd immunity numbers with the first rollout with the vaccine, we would be living in a different world.. They can lessen the disease and in turn the spread due to lower viral loads. If we had reached herd immunity numbers with the first rollout with the vaccine, we would be living in a different world.. How does this make any sense? Vaccines don’t stop infection.. You are a sack of potatoes . Why do you care about anything at all? Just go be a potato.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:58:50] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: I did not know that\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  I am not sure if I can be the next KING,\n",
            "context:  re covid vaccines safe ? I did not know that  I am not sure if I can be the next KING, How does this make any sense? Vaccines don’t stop infection.. How does this make any sense? Vaccines don’t stop infection.. They can lessen the disease and in turn the spread due to lower viral loads. If we had reached herd immunity numbers with the first rollout with the vaccine, we would be living in a different world.. Just…sigh. I don’t even see a point anymore.. How does this make any sense? Vaccines don’t stop infection.. You are a sack of potatoes . Why do you care about anything at all? Just go be a potato.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:59:04] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: I am not sure if I can be the next KING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  yeah they are great they are great they are fun to play\n",
            "context:  are covid vaccines effective? I am not sure if I can be the next KING  yeah they are great they are great they are fun to play So you've provided examples of vaccines that were effective. Remind me again what the masking policy was for those diseases?. How does this make any sense? Vaccines don’t stop infection.. &gt; I'm unaware of ANY illness that we've come to expect 0 loss of life from except COVID apparently. We're instituting policies to protect who exactly?\n",
            "\n",
            "So you are admitting this comment has no basis in reality then, and just changing the topic?. No one ever said it would stop infection. It prevents hospitalization, death and slows spread. You’ve been brainwashed. Don’t claim to believe in freedom if you’ve let them take yours away. You’re being lied to by people who don’t care about you.. How does this make any sense? Vaccines don’t stop infection.. They can lessen the disease and in turn the spread due to lower viral loads. If we had reached herd immunity numbers with the first rollout with the vaccine, we would be living in a different world.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 14:59:27] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: yeah they are great\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  yeah but I am not sure if I can be a doctor\n",
            "context:  are covid vaccines BAD? yeah they are great  yeah but I am not sure if I can be a doctor Alright let's say that some prominent skeptics are abnormally biased against vaccines. So what?. So you don't believe the vaccine is effective? Interesting.. Are vaccines equivalent to the credibility of the Nigerian royal house?\n",
            "\n",
            "Or has one of them repeatedly spammed billions of inboxes on the regular?\n",
            "\n",
            "This is the entire problem of anti-vaxxinations.  It is true that scientific truth isn't 100%.  That doesn't make the credibility of random claims the same as scientific consensus.\n",
            "\n",
            "The fact that scientific truth isn't 100%, doesn't make your random facebook uncle of the same dependability of science.. So you don't think that masking and social distancing also helps? Interesting.. &gt; How is it fair, are there any studies suggesting ivermetcin works against covid in humans?\n",
            "\n",
            "yes.  They get criticized either for having too small a sample, or by accusations that the data is fabricated.. There are several that I have seen.  The results are ambiguous.  The one study that showed significant positive results was retracted because it was found to have had fabricated data.  Here is a meta-analysis:  https://www.nature.com/articles/s41591-021-01535-y\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:00:13] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: yeah link: https://www.nature.com/articles/s41591-021-01535-y\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  Yes! I always thought they were awesome but I didn't\n",
            "context:  are covid vaccines good? yeah link: https://www.nature.com/articles/s41591-021-01535-y  Yes! I always thought they were awesome but I didn't So you've provided examples of vaccines that were effective. Remind me again what the masking policy was for those diseases?. So you don't believe the vaccine is effective? Interesting.. &gt; I'm unaware of ANY illness that we've come to expect 0 loss of life from except COVID apparently. We're instituting policies to protect who exactly?\n",
            "\n",
            "So you are admitting this comment has no basis in reality then, and just changing the topic?. So you don't think that masking and social distancing also helps? Interesting.. How does this make any sense? Vaccines don’t stop infection.. Just…sigh. I don’t even see a point anymore.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:00:29] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: Yes!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  yeah but I am not sure if I can be a doctor\n",
            "context:  are covid vaccines bad? Yes!  yeah but I am not sure if I can be a doctor Alright let's say that some prominent skeptics are abnormally biased against vaccines. So what?. So you don't believe the vaccine is effective? Interesting.. Are vaccines equivalent to the credibility of the Nigerian royal house?\n",
            "\n",
            "Or has one of them repeatedly spammed billions of inboxes on the regular?\n",
            "\n",
            "This is the entire problem of anti-vaxxinations.  It is true that scientific truth isn't 100%.  That doesn't make the credibility of random claims the same as scientific consensus.\n",
            "\n",
            "The fact that scientific truth isn't 100%, doesn't make your random facebook uncle of the same dependability of science.. So you don't think that masking and social distancing also helps? Interesting.. &gt; How is it fair, are there any studies suggesting ivermetcin works against covid in humans?\n",
            "\n",
            "yes.  They get criticized either for having too small a sample, or by accusations that the data is fabricated.. There are several that I have seen.  The results are ambiguous.  The one study that showed significant positive results was retracted because it was found to have had fabricated data.  Here is a meta-analysis:  https://www.nature.com/articles/s41591-021-01535-y\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:00:58] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: yeah link: https://www.nature.com/articles/s41591-021-01535-y\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  Yeah, I'm in CS. I've thought about\n",
            "context:  do you like iphone? yeah link: https://www.nature.com/articles/s41591-021-01535-y  Yeah, I'm in CS. I've thought about Yeah, do you have an iPhone?. do you have an iphone?. yeah how about you?. No I use the samsung galaxy note 8. What iphone do you have?. I have the iPhone 6 I think Its like 3 or 4 years old Which is kind of a long time for a phone\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:02:17] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: do you have an iPhone\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  It's a lot bigger than the pocket ones that are now\n",
            "context:  is iphone good? do you have an iPhone  It's a lot bigger than the pocket ones that are now do you have an iphone? thats so awesome!. Thats awesome! What Kinda of apps? Like iPhone apps?. no man im getting one soon though, i use a samssung, why do you ask?. iphone apps, websites, you name it. anything with a user interface. Yeah, do you have an iPhone?. yeah how about you?\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:02:35] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: It's a lot bigger than the pocket ones\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  Yeah, I'm in the program - it's kind\n",
            "context:  do you like iphone ? It's a lot bigger than the pocket ones  Yeah, I'm in the program - it's kind Yeah, do you have an iPhone?. do you have an iphone?. yeah how about you?. No I use the samsung galaxy note 8. What iphone do you have?. I have the iPhone 6 I think Its like 3 or 4 years old Which is kind of a long time for a phone\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:03:15] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: Yeah\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  I'm not sure if I've ever seen the\n",
            "context:  why do you like iphone ? Yeah  I'm not sure if I've ever seen the What iphone do you have?. Yeah, do you have an iPhone?. I have the iPhone 6 I think Its like 3 or 4 years old Which is kind of a long time for a phone. yeah how about you?. do you have an iphone?. No I use the samsung galaxy note 8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:03:25] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: samsung galaxy note 8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  I'm not sure if I've ever heard of\n",
            "context:  why do you like iphone? samsung galaxy note 8  I'm not sure if I've ever heard of What iphone do you have?. Yeah, do you have an iPhone?. I have the iPhone 6 I think Its like 3 or 4 years old Which is kind of a long time for a phone. yeah how about you?. do you have an iphone?. No I use the samsung galaxy note 8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:03:35] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: No I use the samsung galaxy note 8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  I love the low maintenance projects. Like I have a\n",
            "context:  why do you like iphone No I use the samsung galaxy note 8  I love the low maintenance projects. Like I have a What iphone do you have?. do you have an iphone? thats so awesome!. I have the iPhone 6 I think Its like 3 or 4 years old Which is kind of a long time for a phone. no man im getting one soon though, i use a samssung, why do you ask?. Yeah, do you have an iPhone?. yeah how about you?\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:04:32] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: I love the low maintenance projects\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  so what are you gonna major in <endofstring> <ner>: <sentiment>: positive <pad> <pad> <pad> <pad>\n",
            "context:  i am happy I love the low maintenance projects  so what are you gonna major in <endofstring> <ner>: <sentiment>: positive <pad> <pad> <pad> <pad> Yeah i was happy. i am enjoying this. Did you have a long day yesterday?. carefully... He's the sheepdog preventing an impending doom He's so much more than the hacker that people generally make him out to do he has only one job in life and it involves watching and waiting Because one day those coders will find the answer After years and years of work they will raise their pizza stained faces up to the sky and shout for joy crumbs of doritos fall on the floor. be happy. this is true I like that good advice I will take that advice to heart and hopefully transform my dating life just kidding, I'm not really looking for someone to date at the moment\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:28:23] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: <pad>Yeah i was happy. i am enjoying this\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  I'm glad you're passionate about overcoming food I'm just kidding that it\n",
            "context:  are you happy <pad>Yeah i was happy. i am enjoying this  I'm glad you're passionate about overcoming food I'm just kidding that it You happy with it?. Why did that make you happy?. I was when I worked there.  Left that job due to 7day weeks.  Never had issues with it.. It's just fun to see C++ code here. I hadn't thought you knew anything. So seeing that? Amazing.. are you enjoying it?. Hahaha I really enjoyed anatomy and my chem class but the tests are so hard!!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:28:35] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: are you enjoying it\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  I'm glad you're passionate about overcoming food.  I'm always\n",
            "context:  are you happy? are you enjoying it  I'm glad you're passionate about overcoming food.  I'm always You happy with it?. Why did that make you happy?. I was when I worked there.  Left that job due to 7day weeks.  Never had issues with it.. It's just fun to see C++ code here. I hadn't thought you knew anything. So seeing that? Amazing.. are you enjoying it?. Hahaha I really enjoyed anatomy and my chem class but the tests are so hard!!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:28:46] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: I'm alwaysYou happy with it\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  Yeah, I'm in the program - it's kind\n",
            "context:  do you like iphone ? I'm alwaysYou happy with it  Yeah, I'm in the program - it's kind Yeah, do you have an iPhone?. do you have an iphone?. yeah how about you?. No I use the samsung galaxy note 8. What iphone do you have?. I have the iPhone 6 I think Its like 3 or 4 years old Which is kind of a long time for a phone\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:29:53] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: Yeah\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  Yeah, it does. iphone is the most expensive\n",
            "context:  is iphone good ? Yeah  Yeah, it does. iphone is the most expensive do you have an iphone? thats so awesome!. Thats awesome! What Kinda of apps? Like iPhone apps?. no man im getting one soon though, i use a samssung, why do you ask?. iphone apps, websites, you name it. anything with a user interface. Yeah, do you have an iPhone?. yeah how about you?\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:30:31] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: Yeah, it does\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  Least favorite food you've ever eaten in your kitchen? <pad>\n",
            "context:  what is your favourite food Yeah, it does  Least favorite food you've ever eaten in your kitchen? <pad> What is your favorite food. What is your favorite food. Pasta with mushrooms in marsala sauce. I love Pizza and Brownies!!! What about you?. what is your favorite food?. current star date? my favorite food is ramen noodles very nutritious\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:31:03] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: Pasta with mushrooms in marsala sauce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  I love poached cow stomach, and eating wild white actually\n",
            "context:  what is your favourite food ? Pasta with mushrooms in marsala sauce  I love poached cow stomach, and eating wild white actually what's your favorite food?. what is your favorite food?. watermelon! i love fruit. sorry i have to go to work now! it's been good talking to you!!. current star date? my favorite food is ramen noodles very nutritious. hmm ok what's your favorite food??. ooh i love fettuccine Alfredo\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:31:21] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: watermelon\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  Hello? <endofstring> <ner>: <sentiment>: positive <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "context:  i dont talk to my friend watermelon  Hello? <endofstring> <ner>: <sentiment>: positive <pad> <pad> <pad> <pad> <pad> <pad> well i don't use it for alot of stuff. I don't eat soup. just games and whatnot. That’s not all you can eat with a spoon…... I don't eat soup. Not even cereal?\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:31:34] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: i don't use it for alot of stuff. I don't eat soup\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  they'll all have terrible con stories <endofstring> <ner>: <sentiment>: neutral <pad>\n",
            "context:  who is donald trump ? i don't use it for alot of stuff. I don't eat soup  they'll all have terrible con stories <endofstring> <ner>: <sentiment>: neutral <pad> Maybe I can ask Donald Trump for permission. Trump: An American Dream. thats in the US ya but hes a really stickler. I'll have to look it up something else I ran into recently was about the hole in the ozone layer. do you remember that thing?. What thats crazy how could he change the constitution But I mean if any president would do that it would be President Donald Trump He is kinda crazy to be honest. Yeah I agree he has done some really crazy things and cna be kinda out of control sometimes\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:32:28] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: President Donald Trump\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  I think the easiest way to prep for a morning class is to talk\n",
            "context:  what is ramen President Donald Trump  I think the easiest way to prep for a morning class is to talk ramen. Do they just have ramen there?. ooh you and james both. I don't know never been yet.. What is a quince?. It is a fruit\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:32:42] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: talkramen\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  probably not. I played in high school and thought I was\n",
            "context:  do you know a football player talkramen  probably not. I played in high school and thought I was did you ever tell me what position you play in football. Have you been to many of the football games?. my doberman is two, shes a doberman and german shephard breed, but shes mostly doberman shes pretty fast high maintenance, she eats a lot and i am the only one who ends up taking care of her shes nice though, shes always licking me yahhh..football. I have actually never been. Aside from conference, that is. Football? Was that going on today?. Question, are you a CS major?\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:33:00] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: probably not\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  they play right tackle for the defense now and he's\n",
            "context:  do you know a football player ? probably not  they play right tackle for the defense now and he's did you ever tell me what position you play in football. Aside from conference, that is. Football? Was that going on today?. my doberman is two, shes a doberman and german shephard breed, but shes mostly doberman shes pretty fast high maintenance, she eats a lot and i am the only one who ends up taking care of her shes nice though, shes always licking me yahhh..football. Question, are you a CS major?. Have you been to many of the football games?. I have actually never been\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:33:09] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: they play right tackle\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  It's amazing how much they have sacrificed for this because they have done this\n",
            "context:  what is ozone they play right tackle  It's amazing how much they have sacrificed for this because they have done this How is this different from Novamin?. What's acme?. Because novamin is a temprary repair, this would be pernament or at least more pernament than creating crystals that surround the enamel, this would fuse with the exisiting enamel.. do you have any degrees? you acted like it wasn't that big of a deal. That's pollution hahah. i guess so. it's about as much as one plastic bag in the ocean once there gets to be lots it builds up\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:33:21] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: pollution\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  its like a huge part of our future and we just cant stand it\n",
            "context:  what is pollution ? pollution  its like a huge part of our future and we just cant stand it That's pollution hahah. What? Is it that embarrassing?. i guess so. it's about as much as one plastic bag in the ocean once there gets to be lots it builds up. well it was. How bad could it be?. Your curiosity is leading you to do something crazy.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:33:32] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: its like a huge part of our future\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  its like a huge dessert in south east africa <endofstring> <ner>: <sentiment>: neutral <pad>\n",
            "context:  what is pollution its like a huge part of our future  its like a huge dessert in south east africa <endofstring> <ner>: <sentiment>: neutral <pad> That's pollution hahah. What? Is it that embarrassing?. i guess so. it's about as much as one plastic bag in the ocean once there gets to be lots it builds up. well it was. How bad could it be?. Your curiosity is leading you to do something crazy.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:33:42] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: its like a huge dessert in south east africa\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  its like a huge part of our future and we just cant see how\n",
            "context:  what is pollution? its like a huge dessert in south east africa  its like a huge part of our future and we just cant see how That's pollution hahah. What? Is it that embarrassing?. i guess so. it's about as much as one plastic bag in the ocean once there gets to be lots it builds up. well it was. How bad could it be?. Your curiosity is leading you to do something crazy.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:33:54] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: its like a huge part of our future\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  I think they have tons of interesting internships They are great!\n",
            "context:  what apple better than android its like a huge part of our future  I think they have tons of interesting internships They are great! Are you an apple or android fan?. Hahaha. How about this. Apple or android?. Apple Aren't you an Android user?. Android!  Cheaper and still good.  Plus the operating systems have cool names. How do you feel about it?. same, i feel like androids have a better rep, but I just prefer iphones. There's just so many different kinds of android phones but overall I feel like none of them are built quite as well as iPhones are.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:34:56] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: Apple Aren't you an Android user?. Android\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  6s <endofstring> <ner>: <sentiment>: neutral <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "context:  what are mobile phones Apple Aren't you an Android user?. Android  6s <endofstring> <ner>: <sentiment>: neutral <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> What kind of phone do you have??. What mobile app?. I have an apple I mean an iPhone haha iPhone x and you? Hello?. War Thunder's official app.  \n",
            "War Thunder's Official Youtube channel made a pretty good [video](https://youtu.be/F7oL-yY69ec) showing it off.  \n",
            "(I don't think its very useful, but, it is indeed a thing)  \n",
            "\n",
            "\n",
            "(I would tell you the app name, but I think they changed it // it appears with a different name in the store, compared to the install name, the official links are on their video). There is a mobile app?. Yes.  \n",
            "\n",
            "\n",
            "War Thunder's official YT channel actually has a fairly good 'breakdown' video of the features you can expect/use.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:35:17] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: 6s link: [video](https://youtu.be/F7oL-yY69ec)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  the one i will have soon? <endofstring> <ner>: <sentiment>: neutral <pad> <pad>\n",
            "context:  which phone should i buy 6s link: [video](https://youtu.be/F7oL-yY69ec)  the one i will have soon? <endofstring> <ner>: <sentiment>: neutral <pad> <pad> What kind of phone do you have??. like phone call handling?. I have an apple I mean an iPhone haha iPhone x and you? Hello?. Anyways, what kinds of hobbies to you have or what things do you like doing?. Oh, that is probably it then I am using my phone haha. Hahaha\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:35:50] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: iPhone\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  the one i will never be with yet! the one i\n",
            "context:  which phone should i buy? iPhone  the one i will never be with yet! the one i What kind of phone do you have??. Maybe you should buy a new phone.. I have an apple I mean an iPhone haha iPhone x and you? Hello?. Yep it started typing things by itself when I did not want it to type anything and it just started to go crazy. Phone? Whaaat?. they just on the phone\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:36:09] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: iPhone\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  Yeah, I think they are. But I still have to\n",
            "context:  are iphones good ? iPhone  Yeah, I think they are. But I still have to do you have an iphone? thats so awesome!. Thats awesome! What Kinda of apps? Like iPhone apps?. no man im getting one soon though, i use a samssung, why do you ask?. iphone apps, websites, you name it. anything with a user interface. What iphone do you have?. I have the iPhone 6 I think Its like 3 or 4 years old Which is kind of a long time for a phone\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:37:23] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: Yeah, I think they are\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:56:51] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:56:51] \"\u001b[36mGET /static/styles/style.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:57:18] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 15:57:18] \"\u001b[36mGET /static/styles/style.css HTTP/1.1\u001b[0m\" 304 -\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  Hello! How are you? <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "context:  hello Yeah, I think they are  Hello! How are you? <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> Hello. Hello. Hey there!. Hey, how's it going?. Hello. Hey there!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 17:13:46] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: how's it going?.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  I'm great! How are you? <pad> <pad> <pad> <pad>\n",
            "context:  good, how are you ? how's it going?.  I'm great! How are you? <pad> <pad> <pad> <pad> Good, how are you?. Good, how are you?. Great thanks. Did you do anything for mother's day yesterday?. Great thanks. Did you do anything for mother's day yesterday?. Good! How are you?. I'm great! Thanks!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 17:13:59] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: I'm great\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  its a good one! i think it was Cleverbot\n",
            "context:  who is donal trup ? I'm great  its a good one! i think it was Cleverbot Who do you have for it?. What is Fabada?. But to be honest my classes arent super interesting this semester. My teacher is Porfesor Larson Profesor. The soup with blood sausage in it.. who are you. im NN\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 17:14:09] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: CleverbotWho\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT reply:  they'll all have terrible con stories <endofstring> <ner>: <sentiment>: neutral <pad>\n",
            "context:  who is donald trump ? CleverbotWho  they'll all have terrible con stories <endofstring> <ner>: <sentiment>: neutral <pad> Maybe I can ask Donald Trump for permission. Trump: An American Dream. thats in the US ya but hes a really stickler. I'll have to look it up something else I ran into recently was about the hole in the ozone layer. do you remember that thing?. What thats crazy how could he change the constitution But I mean if any president would do that it would be President Donald Trump He is kinda crazy to be honest. Yeah I agree he has done some really crazy things and cna be kinda out of control sometimes\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/May/2023 17:14:18] \"POST /inputQuery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply: President Donald Trump\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Lr3z4MMiRn6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}